{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90e709f7-2663-4c6c-89e6-9c7f4fc16a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "932d805b-0515-46e5-a01b-33aa74e8cfd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('haberler.txt', encoding='utf8') as f:\n",
    "    \n",
    "    txt_sentences = f.readlines()\n",
    "    \n",
    "new_list = ''.join(txt_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "155008dd-afce-4e08-8985-77d74a4f4ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('turkce-stop-words', encoding='utf-8') as f:\n",
    "    \n",
    "    txt_stop_words = f.readlines()\n",
    "    \n",
    "stop_word_list = [i[:-1] for i in txt_stop_words]  # sondaki '\\n' atmak için"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "686abcb4-0722-4332-b34e-b90f9cd44384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(corpus, windows_size, stop_word_list):\n",
    "    \"\"\"\n",
    "        corpus: metin\n",
    "        windows_size: kaç komşu kelime\n",
    "        embedding_size: kaç boyutlu vektör ile temsil edilecek\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # remove punctuations and make lower corpus\n",
    "    corpus = re.sub(r'\\\\n', ' ', corpus)  # \\\\n temizle\n",
    "    corpus = re.sub(r'\\n', ' ', corpus)  # \\n temizle\n",
    "    corpus = re.sub(r'[^\\w\\s]', ' ', corpus)  # word ve space hariç her şeyi at\n",
    "    corpus = corpus.lower()  # küçük hale getir\n",
    "    corpus = ' '.join([i for i in corpus.split() if i not in stop_word_list])  # remove stop words\n",
    "    corpus = re.sub(r\" \\d+\", \" \", corpus)  # sayıları temizle\n",
    "    corpus =  ' '.join([i for i in corpus.split() if len(i)>1])  # tek harfleri kaldır. (H.C(isim soyisim)-> h c -> anlamsız)\n",
    "    corpus = re.sub(r' +', ' ', corpus)  # büyük space alanlarını tek space haline getir\n",
    "    \n",
    "    \n",
    "    \n",
    "    # word list\n",
    "    corpus_word_list = corpus.split()\n",
    "    corpus_word_list_len = len(corpus_word_list)\n",
    "    \n",
    "    \n",
    "    # find number of unique value and assign a number to them\n",
    "    unique_words = set(corpus_word_list)\n",
    "    number_of_unique_words = len(unique_words)\n",
    "    \n",
    "    word_to_token_dict = {}\n",
    "    token_to_word_dict = {}\n",
    "    for token,word in enumerate(unique_words):\n",
    "        word_to_token_dict[word] = token+1  # +1 olmasının sebebi 1 den başlasın, 0 padding için kullanılsın.\n",
    "        token_to_word_dict[token+1] = word\n",
    "    \n",
    "    \n",
    "    dataframe = []\n",
    "    \n",
    "    for i, word in enumerate(corpus_word_list):\n",
    "        \n",
    "        context_words = corpus_word_list[i-windows_size:i]+corpus_word_list[i+1:i+1+windows_size]\n",
    "        context_words_token = [word_to_token_dict[word] for word in context_words]\n",
    "        \n",
    "        while len(context_words_token) < 4:  # padding\n",
    "            context_words_token.append(0)\n",
    "            \n",
    "        dataframe.append([context_words_token, word_to_token_dict[word]])\n",
    "        \n",
    "        \n",
    "    # örnek gösterim için\n",
    "    print(' EXAMPLE : \\n\\n')\n",
    "    print(' '.join(corpus_word_list[2:7]))\n",
    "    print('\\n --------------- \\n')\n",
    "    \n",
    "    for i in range(4,5):\n",
    "        print(str([token_to_word_dict[a] for a in dataframe[i][0]])+ ' --> ' +str(token_to_word_dict[dataframe[i][1]]) +'\\n')\n",
    "        print(str([dataframe[i][0]])+ ' --> ' +str(dataframe[i][1]) +'\\n\\n')\n",
    "        \n",
    "    print(f\"\\nnumber_of_unique_words : {number_of_unique_words}\\n\")\n",
    "        \n",
    "    dataframe = [[torch.tensor(i), torch.tensor(y)] for i,y in dataframe]  # convert dataframe elemnt to tensor (this is list of tensors)\n",
    "    \n",
    "    context_list = [i for i, y in dataframe]  # split context and targets\n",
    "    target_list = [y for i, y in dataframe]\n",
    "    \n",
    "    context_tensor = torch.stack(context_list)\n",
    "    target_tensor = torch.stack(target_list)\n",
    "    \n",
    "    dataset = TensorDataset(context_tensor,target_tensor)  # create dataset and dataloader\n",
    "    dataloader = DataLoader(dataset) \n",
    "    \n",
    "    return dataset, number_of_unique_words, word_to_token_dict, token_to_word_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e977f55a-71ec-499a-a654-983b8c20f824",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW_Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embeddings, embedding_dim, windows_size):\n",
    "        super(CBOW_Model, self).__init__()\n",
    "        \n",
    "        self.embedding_layer = nn.Embedding(num_embeddings+1, embedding_dim)  # num_embeddings: kaç farklı kelime var, embedding_dim: vector kaç boyutlu\n",
    "        self.fc1 = nn.Linear(embedding_dim*windows_size*2,128)  # 5*2*2 = 20 boyulu giriş (bu örnek için 20) # hidden layer\n",
    "        self.fc2 = nn.Linear(128, num_embeddings+1)  # output layer  # çıktı output unique sayısı kadar olmalı\n",
    "        \n",
    "        \n",
    "    def forward(self, x_in):\n",
    "        out = self.embedding_layer(x_in).view(1,-1).squeeze(0)  # 5 boyutlu 4 vektörden 20 boyutlu tek vektöre\n",
    "        out = self.fc1(out)  # 5 boyutlu çıkış\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = F.log_softmax(out, dim=-1)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def get_word_vector(self, word_tokenize):\n",
    "        word = (torch.LongTensor([word_tokenize]))\n",
    "        return self.embedding_layer(word).view(1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a92843eb-0750-4a26-8fa5-cedfc69cd59b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " EXAMPLE : \n",
      "\n",
      "\n",
      "dolar tl üçüncü çeyrekte görecek\n",
      "\n",
      " --------------- \n",
      "\n",
      "['dolar', 'tl', 'çeyrekte', 'görecek'] --> üçüncü\n",
      "\n",
      "[[1958, 528, 2226, 540]] --> 1470\n",
      "\n",
      "\n",
      "\n",
      "number_of_unique_words : 2889\n",
      "\n",
      "1\n",
      "Epoch: 1, Loss: 6.8381524085998535, Validation Loss: 8.414573669433594\n",
      "2\n",
      "Epoch: 2, Loss: 7.466350555419922, Validation Loss: 7.775332450866699\n",
      "3\n",
      "Epoch: 3, Loss: 7.577645301818848, Validation Loss: 6.829939842224121\n",
      "4\n",
      "Epoch: 4, Loss: 7.8071208000183105, Validation Loss: 5.842516899108887\n",
      "5\n",
      "Epoch: 5, Loss: 7.9753618240356445, Validation Loss: 5.209146499633789\n",
      "6\n",
      "Epoch: 6, Loss: 7.877024173736572, Validation Loss: 6.623602390289307\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' ------  '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" TRAINING CELL  \"\"\"\n",
    "\n",
    "with open('uzun_haberler.txt', encoding='utf8') as f:\n",
    "    \n",
    "    txt_sentences = f.readlines()\n",
    "    \n",
    "corpus = ''.join(txt_sentences)\n",
    "\n",
    "corpus = corpus[:50000] # çok uzun sürüyor \n",
    "\n",
    "windows_size = 2\n",
    "\n",
    "dataframe, number_of_unique_words, word_to_token_dict, token_to_word_dict = create_dataset(corpus, windows_size, stop_word_list)\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(dataframe, [int(len(dataframe) * 0.7), len(dataframe) - int(len(dataframe) * 0.7)])  # %70 train\n",
    "val_set, test_set = torch.utils.data.random_split(val_set, [int(len(val_set) * 0.5), len(val_set) - int(len(val_set) * 0.5)])  # %15 val, %15 test\n",
    "\n",
    "model = CBOW_Model(num_embeddings=number_of_unique_words, embedding_dim=5, windows_size=windows_size)\n",
    "\n",
    "\n",
    "n_epochs = 6\n",
    "loss_f = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) \n",
    "    \n",
    "\n",
    "counter = 0\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    all_losses = []\n",
    "    all_losses_val = []\n",
    "\n",
    "    for context, target in train_set:\n",
    "\n",
    "        pred = model(context)\n",
    "        loss = loss_f(pred.unsqueeze(0), target.unsqueeze(0))\n",
    "\n",
    "        all_losses.append(loss)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    for context_val, target_val in val_set: \n",
    "        \n",
    "        with torch.no_grad():\n",
    "        \n",
    "            pred_val = model(context_val)\n",
    "\n",
    "            loss_val = loss_f(pred_val.unsqueeze(0), target_val.unsqueeze(0))\n",
    "\n",
    "            all_losses_val.append(loss_val)\n",
    "        \n",
    "    \n",
    "    print(epoch)\n",
    "    print(f\"Epoch: {epoch}, Loss: {loss}, Validation Loss: {loss_val}\")\n",
    "    \n",
    "    \n",
    "    if epoch > 5: \n",
    "        if counter > 5:\n",
    "            print('--- Early Stop ---')\n",
    "            break\n",
    "            \n",
    "        if loss_val >= loss_val_backup:\n",
    "            counter +=1\n",
    "        else:\n",
    "            counter = 0\n",
    "            \n",
    "        loss_val_backup = loss_val\n",
    "        \n",
    "    else:\n",
    "        loss_val_backup = loss_val\n",
    "    \n",
    "\n",
    "\"\"\" ------  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a334df1a-63ba-4bb0-b370-03f39495899c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model kelimeleri tamamen rasgele bir şekilde seçse alacağı sonuç : % 0.034614053305642094\n"
     ]
    }
   ],
   "source": [
    "print(f\"model kelimeleri tamamen rasgele bir şekilde seçse alacağı sonuç : % {(1/number_of_unique_words)*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74668c8e-83fa-49b4-a259-b5a92565e95a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy : ½ 2.5974025974025974\n"
     ]
    }
   ],
   "source": [
    "\"\"\" TESTING MODEL ACCURACY \"\"\"\n",
    "\n",
    "\"\"\" Doğru kelimeyi kaç kere bildi.\"\"\"\n",
    "\n",
    "total_true = 0\n",
    "\n",
    "for context_test, target_test in test_set:\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        pred = model(context_test)\n",
    "\n",
    "        if bool(torch.argmax(pred) == target_test):\n",
    "\n",
    "            total_true +=1\n",
    "\n",
    "        acc = (total_true / len(test_set)) * 100\n",
    "        \n",
    "    \n",
    "print(f\"Test Accuracy : ½ {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81c077c2-0645-46d3-80d8-cee6228f1eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'takım'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deneme_cumle = torch.tensor([word_to_token_dict['belediye'], word_to_token_dict['başkan'], word_to_token_dict['üye'], word_to_token_dict['parti']])\n",
    "pred_percentage = model(deneme_cumle)\n",
    "token_to_word_dict[int(torch.argmax(pred_percentage))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bc3afef-43eb-4a39-9f00-5b003b2e553d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.71029663  0.02827632  0.48388532  0.7006433   1.5753489 ]]\n",
      "[[-1.0905299  -0.3770474  -1.4081682   2.6060262   0.40407977]]\n",
      "\n",
      "benzerlik : 0.41220712661743164\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "word1 = model.get_word_vector(word_to_token_dict['parti']).detach().numpy()\n",
    "word2 = model.get_word_vector(word_to_token_dict['üye']).detach().numpy()\n",
    "\n",
    "print(word1)\n",
    "print(word2)\n",
    "\n",
    "print(f\"\\nbenzerlik : {cosine_similarity(word1,word2)[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d162aa5e-1502-4657-ba5c-89ae1aa4dc1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6e18f3-ddf4-4511-9eab-ffd10f1e8088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad62adef-4ad6-40f9-84d4-c6ea0cc36e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccda80b-5546-4bb5-aa8d-06723cf00f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35ea430-e9cf-4690-a369-ff2baa0e4ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c69803-60de-4848-9943-86d67dc96a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851d7d20-d1c6-4634-a7f6-df22b4fbea43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
